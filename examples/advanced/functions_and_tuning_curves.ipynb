{
 "metadata": {
  "kernelspec": {
   "display_name": "nengodev",
   "language": "python",
   "name": "nengodev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "import nengo\n",
      "\n",
      "%load_ext nengo.ipynb\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Improving function approximation with adjustment of tuning curves\n",
      "\n",
      "This tutorial shows how adjusting the tuning curves of neurons can help to implement specific functions with Nengo. As an example we will try to to compute the Heaviside function $\\Theta(x)$ that is 1 for all $x > 0$ and 0 otherwise."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The naive approach\n",
      "\n",
      "As a first pass we just use an ensemble with the default parameters and try to implement the Heaviside function purely with the decoders being solved for."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "n_neurons = 150\n",
      "duration = 2."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def stimulus_fn(t):\n",
      "    return (2. * t / duration) - 1.\n",
      "\n",
      "def heaviside(x):\n",
      "    return x > 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Network() as model_naive:\n",
      "    stimulus = nengo.Node(stimulus_fn)\n",
      "    ens = nengo.Ensemble(n_neurons=n_neurons, dimensions=1)\n",
      "    output = nengo.Node(size_in=1)\n",
      "    \n",
      "    nengo.Connection(stimulus, ens)\n",
      "    nengo.Connection(ens, output, function=heaviside)\n",
      "    \n",
      "    p_naive = nengo.Probe(output, synapse=0.005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Simulator(model_naive) as sim_naive:\n",
      "    sim_naive.run(duration)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = sim_naive.trange()\n",
      "plt.figure()\n",
      "plt.plot(t, sim_naive.data[p_naive], label=\"naive\")\n",
      "plt.plot(t, heaviside(stimulus_fn(t)), '--', c='black', label=\"ideal\")\n",
      "plt.xlabel(\"t\")\n",
      "plt.ylabel(\"Output\")\n",
      "plt.legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that this approach does work, but there is room for improvement."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Investigating the tuning curves\n",
      "\n",
      "Let us take a look at the tuning curves of the neurons in the ensemble."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure()\n",
      "plt.plot(*nengo.utils.ensemble.tuning_curves(ens, sim_naive))\n",
      "plt.xlabel(\"Input\")\n",
      "plt.ylabel(\"Firing rate [Hz]\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "About half of these neurons are tuned to fire more for smaller values. But these values are not really relevant as we are decoding 0 for values below 0. Thus, we change all neurons to be tuned to fire more for larger values by setting all the encoders to be positive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Network() as model_pos_enc:\n",
      "    stimulus = nengo.Node(stimulus_fn)\n",
      "    ens = nengo.Ensemble(n_neurons=n_neurons, dimensions=1,\n",
      "                         encoders=nengo.dists.Choice([[1.]]))\n",
      "    output = nengo.Node(size_in=1)\n",
      "    \n",
      "    nengo.Connection(stimulus, ens)\n",
      "    nengo.Connection(ens, output, function=heaviside)\n",
      "    \n",
      "    p_pos_enc = nengo.Probe(output, synapse=0.005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Simulator(model_pos_enc) as sim_pos_enc:\n",
      "    sim_pos_enc.run(duration)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting tuning curves:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure()\n",
      "plt.plot(*nengo.utils.ensemble.tuning_curves(ens, sim_pos_enc))\n",
      "plt.xlabel(\"Input\")\n",
      "plt.ylabel(\"Firing rate [Hz]\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = sim_pos_enc.trange()\n",
      "plt.figure()\n",
      "plt.plot(t, sim_naive.data[p_naive], label=\"naive\")\n",
      "plt.plot(t, sim_pos_enc.data[p_pos_enc], label=\"pos. enc.\")\n",
      "plt.plot(t, heaviside(stimulus_fn(t)), '--', c='black', label=\"ideal\")\n",
      "plt.xlabel(\"t\")\n",
      "plt.ylabel(\"Output\")\n",
      "plt.legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compared to the naive approach the representation of values below th threshold is less noisy, but otherwise there is not any improvement. Even though the tuning curves are now all aligned in the correct direction, they are still covering a lot of irrelevant area. Because all values below the threshold should be 0, there is no need to have neurons tuned to this range. We want to shift all the intercepts to the range $(0.0, 1.0)$.\n",
      "\n",
      "Not only the range of intercepts can be important, but also the distribution of intercepts. Let us take a look at the Heaviside function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs = np.linspace(-1, 1, 100)\n",
      "plt.figure()\n",
      "plt.plot(xs, heaviside(xs))\n",
      "plt.ylim(-0.1, 1.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function is mostly constant except for that large jump at the threshold. The constant parts are easy to approximate and will not need a lot of neural resources, but the highly non-linear jump will require much more neural resources for an accurate representation.\n",
      "\n",
      "Thus, let us try to implement just this thresholding of a one-dimensional scalar in three ways: With a uniform distribution of intercepts (the default), all intercepts at 0 (where we have the non-linearity), and an exponential distribution. The last approach is in between the two extremes of a uniform distribution and placing all intercepts at 0. It will distribute most intercepts close to 0, but some intercepts will still be at larger values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "threshold = 0.\n",
      "\n",
      "with nengo.Network() as model_dists:\n",
      "    stimulus = nengo.Node(stimulus_fn)\n",
      "    ens_uniform = nengo.Ensemble(\n",
      "        n_neurons=n_neurons, dimensions=1,\n",
      "        encoders=nengo.dists.Choice([[1]]),\n",
      "        intercepts=nengo.dists.Uniform(threshold, 1.))\n",
      "    ens_fixed = nengo.Ensemble(\n",
      "        n_neurons=n_neurons, dimensions=1,\n",
      "        encoders=nengo.dists.Choice([[1]]),\n",
      "        intercepts=nengo.dists.Choice([threshold]))\n",
      "    ens_exp = nengo.Ensemble(\n",
      "        n_neurons=n_neurons, dimensions=1,\n",
      "        encoders=nengo.dists.Choice([[1]]),\n",
      "        intercepts=nengo.dists.Exponential(0.15, threshold, 1.))\n",
      "    \n",
      "    out_uniform = nengo.Node(size_in=1)\n",
      "    out_fixed = nengo.Node(size_in=1)\n",
      "    out_exp = nengo.Node(size_in=1)\n",
      "    \n",
      "    nengo.Connection(stimulus, ens_uniform)\n",
      "    nengo.Connection(stimulus, ens_fixed)\n",
      "    nengo.Connection(stimulus, ens_exp)\n",
      "    nengo.Connection(ens_uniform, out_uniform, function=heaviside)\n",
      "    nengo.Connection(ens_fixed, out_fixed, function=heaviside)\n",
      "    nengo.Connection(ens_exp, out_exp, function=heaviside)\n",
      "    \n",
      "    p_uniform = nengo.Probe(out_uniform, synapse=0.005)\n",
      "    p_fixed = nengo.Probe(out_fixed, synapse=0.005)\n",
      "    p_exp = nengo.Probe(out_exp, synapse=0.005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Simulator(model_dists) as sim_dists:\n",
      "    sim_dists.run(duration)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us look at the tuning curves first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(12, 4))\n",
      "\n",
      "plt.subplot(1, 3, 1)\n",
      "plt.plot(*nengo.utils.ensemble.tuning_curves(ens_uniform, sim_dists))\n",
      "plt.xlabel(\"Input\")\n",
      "plt.ylabel(\"Firing rate [Hz]\")\n",
      "plt.title(\"Uniform intercepts\")\n",
      "\n",
      "plt.subplot(1, 3, 2)\n",
      "plt.plot(*nengo.utils.ensemble.tuning_curves(ens_fixed, sim_dists))\n",
      "plt.xlabel(\"Input\")\n",
      "plt.ylabel(\"Firing rate [Hz]\")\n",
      "plt.title(\"Fixed intercepts\")\n",
      "\n",
      "plt.subplot(1, 3, 3)\n",
      "plt.plot(*nengo.utils.ensemble.tuning_curves(ens_exp, sim_dists))\n",
      "plt.xlabel(\"Input\")\n",
      "plt.ylabel(\"Firing rate [Hz]\")\n",
      "plt.title(\"Exponential intercept distribution\")\n",
      "\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us look at how these three ensembles approximate the thresholding function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = sim_dists.trange()\n",
      "plt.figure()\n",
      "plt.plot(t, sim_naive.data[p_naive], label='naive', c='gray')\n",
      "plt.plot(t, sim_dists.data[p_uniform], label='Uniform intercepts')\n",
      "plt.plot(t, sim_dists.data[p_fixed], label='Fixed intercepts')\n",
      "plt.plot(t, sim_dists.data[p_exp], label='Exp. intercept dist.')\n",
      "plt.plot(t, heaviside(stimulus_fn(t)), '--', c='black', label=\"ideal\")\n",
      "plt.xlabel('t')\n",
      "plt.ylabel('Output')\n",
      "plt.legend(loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the fixed intercepts produce slightly higher decoded values close to the threshold, but the slope is lower than for uniform intercepts. The best approximation of the thresholding is done with the exponential intercept distribution. Here we get a quick rise to 1 at the threshold and a fairly constant representation of 1 for value sufficiently above the threshold. All three distributions will perfectly represent values below 0.\n",
      "\n",
      "Nengo provides the `ThresholdingEnsemble` preset to make it easier to assign intercepts according to that distribution as well as adjusting the encoders and evaluation points accordingly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "with nengo.Network() as model_final:\n",
      "    stimulus = nengo.Node(stimulus_fn)\n",
      "    with nengo.presets.ThresholdingEnsembles(0.):\n",
      "        ens = nengo.Ensemble(n_neurons=n_neurons, dimensions=1)\n",
      "    output = nengo.Node(size_in=1)\n",
      "    \n",
      "    nengo.Connection(stimulus, ens)\n",
      "    nengo.Connection(ens, output, function=heaviside)\n",
      "    \n",
      "    p_final = nengo.Probe(output, synapse=0.005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with nengo.Simulator(model_final) as sim_final:\n",
      "    sim_final.run(duration)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = sim_final.trange()\n",
      "plt.figure()\n",
      "plt.plot(t, sim_final.data[p_final], label=\"final\")\n",
      "plt.plot(t, heaviside(stimulus_fn(t)), '--', c='black', label=\"ideal\")\n",
      "plt.xlabel(\"t\")\n",
      "plt.ylabel(\"Output\")\n",
      "plt.legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "The take-away from this tutorial is that adjusting ensemble parameters in the right way can sometimes help in implementing functions more acurately in neurons."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}

